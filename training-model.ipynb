{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d802c6a",
   "metadata": {},
   "source": [
    "# Scrapping Data dengan LLM - berbasis Streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b54d0f4",
   "metadata": {},
   "source": [
    "- penjelasan mengenai project: project ini bertujuan untuk membuat web scrapping berbasis streamlit. dimana nantinya akan menggunakan model seperti LLM, sistem ini nantinya dapat membantu kita salam melakukan scrapping data secara otomatis tanpa menghabiskan banyak waktu.\n",
    "- penjelasan model dan alasan memilih model tersebut: model yang digunakan adalah LLM, tujuan menggunakan model LLM karena model ini dapat memerikan respon yang baik untuk hasil scrapping data yang dilakukan nantinya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38efc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8387c889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mempersiapkan model LLM OLLAMA versi mistral\n",
    "model = OllamaLLM(model=\"mistral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a1391e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template prompt untuk ekstraksi data dari HTML\n",
    "template = (\n",
    "    \"You are tasked with extracting specific information from the following text content: {dom_content}.\\n\"\n",
    "    \"Please follow these instructions carefully:\\n\\n\"\n",
    "    \"1. **Extract Information:** Only extract the information that directly matches the provided description: {parse_description}.\\n\"\n",
    "    \"2. **No Extra Content:** Do not include any additional text, comments, or explanations in your response.\\n\"\n",
    "    \"3. **Empty Response:** If no information matches the description, return an empty string ('').\\n\"\n",
    "    \"4. **Direct Data Only:** Your output should contain only the data that is explicitly requested, with no other text.\\n\"\n",
    "    \"5. **Numerical Data Priority:** Dates, numbers, and quantitative information are important.\\n\"\n",
    "    \"6. **Table Design:** Format table with appropriate headers with each row representing a product.\\n\"\n",
    "    \"7. **CSV Ready:** Output should be in CSV format, with each row representing a product, and the columns for name, price, location, seller.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39801785",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  mengambil HTML dari web yang di inputkan nantinya\n",
    "def scrape_website(website):\n",
    "    print(\"Meluncurkan browser...\")\n",
    "\n",
    "    options = webdriver.ChromeOptions()\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "    try:\n",
    "        driver.get(website)\n",
    "        time.sleep(10)\n",
    "        html = driver.page_source\n",
    "        return html\n",
    "    finally:\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfa7bd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ambil isi <body> dari HTML\n",
    "def extract_body_content(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    body_content = soup.body\n",
    "    return str(body_content) if body_content else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f097c69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  membersihkan halaman web\n",
    "def clean_body_content(body_content):\n",
    "    soup = BeautifulSoup(body_content, \"html.parser\")\n",
    "    for tag in soup([\"script\", \"style\"]):\n",
    "        tag.extract()\n",
    "    cleaned = soup.get_text(separator=\"\\n\")\n",
    "    return \"\\n\".join(line.strip() for line in cleaned.splitlines() if line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "951ba385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mengirim ke model LLM untuk dilakukan proses ekstraksi\n",
    "def split_dom_content(dom_content, max_length=6000):\n",
    "    return [dom_content[i:i+max_length] for i in range(0, len(dom_content), max_length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "622d4f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing dengan model LLM berdasarkan potongan DOM dan deskripsi\n",
    "def parse_with_ollama(dom_chunks, parse_description, max_items=None):\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    chain = prompt | model\n",
    "\n",
    "    parsed_results = []\n",
    "\n",
    "    for i, chunk in enumerate(dom_chunks, 1):\n",
    "        result = chain.invoke({\"dom_content\": chunk, \"parse_description\": parse_description})\n",
    "        parsed_results.append(result)\n",
    "        print(f\"‚úÖ Batch {i}/{len(dom_chunks)} selesai\")\n",
    "\n",
    "    combined_csv = \"\\n\".join(parsed_results)\n",
    "\n",
    "    # Batasi jumlah item jika max_items diberikan\n",
    "    if max_items and max_items > 0:\n",
    "        lines = combined_csv.strip().split(\"\\n\")\n",
    "        header = lines[0]\n",
    "        rows = lines[1:max_items + 1]\n",
    "        limited_csv = \"\\n\".join([header] + rows)\n",
    "        return limited_csv\n",
    "\n",
    "    return combined_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "decda1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meluncurkan browser...\n",
      "‚úÖ Batch 1/3 selesai\n",
      "‚úÖ Batch 2/3 selesai\n",
      "‚úÖ Batch 3/3 selesai\n"
     ]
    }
   ],
   "source": [
    "# Uji Coba\n",
    "\n",
    "# URL yang ingin di-scrape\n",
    "url = \"https://www.olx.co.id/jakarta-selatan_g4000030/mobil-bekas_c198/q-mobil-bekas\"\n",
    "\n",
    "# Deskripsi data yang ingin diekstrak\n",
    "parse_description = \"Extract table with Product Name, Price, Location, Seller.\"\n",
    "\n",
    "# Jumlah maksimum hasil yang ingin ditampilkan\n",
    "max_items = 30  # ‚Üê Diatur oleh user\n",
    "\n",
    "# Proses scraping dan parsing\n",
    "html = scrape_website(url)\n",
    "body = extract_body_content(html)\n",
    "clean_text = clean_body_content(body)\n",
    "chunks = split_dom_content(clean_text)\n",
    "result_csv = parse_with_ollama(chunks, parse_description, max_items=max_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d29d8168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Hasil ekstraksi (30 item) disimpan ke 'output.csv'\n"
     ]
    }
   ],
   "source": [
    "# Simpan hasil ke file CSV\n",
    "with open(\"output.csv\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(result_csv)\n",
    "\n",
    "print(f\"üìÅ Hasil ekstraksi ({max_items} item) disimpan ke 'output.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c0e265",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrapping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
